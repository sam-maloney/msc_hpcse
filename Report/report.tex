% IEEE standard conference template; to be used with:
%   spconf.sty  - LaTeX style file, and
%   IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------

\documentclass[letterpaper]{article}
\usepackage{spconf,amsmath,amssymb,graphicx,float,enumitem,url,algpseudocode}

% Example definitions.
% --------------------
% nice symbols for real and complex numbers
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\C}[0]{\mathbb{C}}

% bold paragraph titles
\newcommand{\mypar}[1]{{\bf #1.}}

% Title.
% ------
\title{Optimization and parallelization of diffusion solvers\\
using alternating direction implicit and random walk methods}
%
% Single address.
% ---------------
\name{Samuel Maloney} 
\address{Department of Mathematics\\ETH Z\"urich\\Z\"urich, Switzerland}

% For example:
% ------------
%\address{School\\
%		 Department\\
%		 Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%		 {School A-B\\
%		 Department A-B\\
%		 Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%		 while at ...}}
%		 {School C-D\\
%		 Department C-D\\
%		 Address C-D}
%

\begin{document}
%\ninept
%
\maketitle
%


% ----------------------------------------------------------------------
\begin{abstract}
Optimized and parallelized code for numerical simulation of the diffusion equation is presented. Two different algorithms, Alternating Direction Implicit (ADI) and Random Walk (RW) are studied. The algorithms are implemented in C++ and verified against an analytic solution, and then scalar optimizations, AVX vectorization, and OpenMP parallelization are carried out. Benchmarking tests are conducted on the Piz Daint cluster to determine single-core performance and runtime improvements, as well as strong and weak scaling characteristics on multiple cores of a single node. Approximate performance gain of 5X for ADI, and runtime speedups of approximately 6X and 3000X for ADI and RW respectively are observed, along with acceptable OpenMP scaling.
\end{abstract}


% ----------------------------------------------------------------------
\section{Background}\label{sec:background}

\mypar{Motivation} 
Diffusion is the process by which a~quantity of interest spreads from regions of high density to regions of low density and are integral in the study of fluids. Diffusion terms arise in all numerical models of computational fluid dynamics, which are used for such problems as aerodynamic design, turbine flows, chemical reaction mixing, and many others. Efficient and accurate simulation codes for diffusion are thus of great importance in many fields.

In this study, two different algorithms to simulate the diffusion process, Alternating Direction Implicit (ADI) and Random Walk (RW), were implemented, optimized, and parallelized. The mathematical formulations for diffusion in general and for each of these two methods specifically are presented next.

\mypar{Mathematical Formulation}
The process of diffusion is driven by it's conentration gradient according to the diffusion equation:
\begin{equation}
\frac{\partial \rho(\mathbf{r},t)}{\partial t} = D\Delta\rho(\mathbf{r},t)
\label{diff}
\end{equation}
where $\rho(\mathbf{r},t)$, with position $\mathbf{r}$ at time $t$, is the quantity of interest and $D$ is the diffusion constant and is a given value for the system. For this study, homogeneous Dirichlet boundary conditions are used at all boundaries. The initial density distribution used for this study is:
\begin{equation}
\rho(x,y,0)=\sin(\pi x)\sin(\pi y)
\end{equation}
and it's corresponding analytic solution, which was used for verification, is:
\begin{equation}
\rho(x,y,t)=\sin(\pi x)\sin(\pi y)e^{-2D\pi^{2}t}
\label{analytic}
\end{equation}
The studied simulation domain is a unit square with $x$ and $y$ each ranging from $0$ to $1$, which is discretized using a uniform grid such that $\delta h=\delta x=\delta y$. A superscript $n$ is used to denote the timestep $t_n=n\delta t$ and subscripts $i$ and $j$ to denote the indices of the nodes in the mesh $x_i=i\delta x$ and $y_j=j\delta y$ such that $\rho_{i,j}^{n}=\rho(x_i,y_j,t_n)$.

\mypar{Alternating Direction Implicit}
The ADI method is an operator splitting scheme based on finite difference discretizations of the derivatives. It works by splitting the each time step into two half tiem steps, in the first of which the time integration is carried out implicitly in the $x$-direction and explicitly in the $y$-direction, and in the second half time step the directions are switched such that the integration in the $x$-direction is explicit and in the $y$-direction is implicit.

Using a second order centered approximation for the spatial derivative in Eq. \eqref{diff} means that each implicit integration step requires only the solution of a tridiagonal linear system, for which the Thomas Algorithm (for formulation see \cite{thomas}) can be used to efficiently compute the result. These tridiagonal systems can be found by considering the discretization equations:
\begin{equation}
\rho_{i,j}^{n+\frac{1}{2}}=\rho_{i,j}^n+\frac{D\delta t}{d\delta x^2}(\rho_{i-1,j}^{n+\frac{1}{2}}-2\rho_{i,j}^{n+\frac{1}{2}}+\rho_{i+1,j}^{n+\frac{1}{2}})
\end{equation}
\begin{equation}
\rho_{i,j}^{n+1}=\rho_{i,j}^{n+\frac{1}{2}}+\frac{D\delta t}{d\delta x^2}(\rho_{i,j-1}^{n+1}-2\rho_{i,j}^{n+1}+\rho_{i,j+1}^{n+1})
\end{equation}
These can be rewritten in the form $\mathbf{A}\rho^*=\rho^{*-\frac{1}{2}}$ where $\mathbf{A}$ is the tridiagonal matrix to be solved at each half timestep.

\mypar{Random Walk}
Diffusion can also be simulated by tracking the random motion of a set of $M$ identical particles with no inertia. A probability $\lambda=D\delta t/\delta x^2$ is defined, such that at each time step, each particle has a $1-4\lambda$ probability of remaining in the same position. If it does not stay, it is then moved to one of the four neighbouring nodes, with an equal probability of moving in any direction. Clearly, one requires $lambda<1/2$ to maintain a positive probability of staying.

The initial conidition for the numer of particles at each grid point $m_{i,j}$ is given as:
\begin{equation}
m_{i,j}^0=\left \lfloor{\frac{M\rho(x_i,y_j,0)}{\iint_\Omega \rho(x,y,0) \,\mathrm{d}x\,\mathrm{d}y}}\right \rfloor 
\end{equation}
And the approximate solution after a given timestep can then be calculated as:
\begin{equation}
\rho_{i,j}^n=\frac{m_{i,j}^n}{M}\iint_\Omega \rho(x,y,0)  \,\mathrm{d}x\,\mathrm{d}y
\end{equation}

% ----------------------------------------------------------------------
\section{Baseline Implementation}\label{sec:baseline}

In this section, the basic implementation and verification of the described algorithms is presented. All code is based on the Diffusion2D class structure of the solution code provided with the course exercises. All code for each version of the class is contained within a single .cpp source file, and includes user defined header files \textit{timer.hpp} and \textit{tsc\_x86.hpp} for measuring runtime in seconds and cycles respectively. The \textit{timer.hpp} file is from the HPCSE I course exercises, and the \textit{tsc\_x86.hpp} file is from the exercises of the How to Write Fast Numerical Code course exercises.

The programs take arguments to set the diffusion constant $D$, size $N$ of one dimension of the grid, timestep $\delta t$, number of repititions $n_{runs}$ to run the simulation, and the number of timesteps $n_{steps}$ to run each simulation for. The RW codes also take a value for the numer of particles $M$ in the simulation. For all codes the values for $n_{runs}$ and $n_{steps}$ are optional, and default to $n_{runs}=1$ and $n_{steps}=0.1/\delta t$ respectively. The ordering of these input arguments can be displayed by running the program with no arguments.

For each simulation run, a new Diffusion2D class object is initialized, with one-time initialization routines happening outside of the timing structures. Once initialization is complete, the simulation is is run to completion by a single call to the {\tt run\_simulation} function, and this call is the only part of the program which is timed and contains all code which will be optimized. 

\mypar{ADI}
An order verification study (OVS) of the ADI code in both spatial and temporal diensions was performed by comparing the simulation results to the analytic solution in Eq. \eqref{analytic}. The results can be seen in Fig. \ref{fig:OVS_ADI} and show the implementation to be second order accurate in both space and time, as expected for this discretization scheme.

The ADI algorithm complexity should be $O(N^2)$ as it does a constant number of operations for updating value at each point in the grid. It's runtime scaling (see \ref{subsec:ADI_results}, Fig. \ref{fig:runtime_ADI}) is also found to agree with this expected complexity.

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/OVS_ADI.eps}
  \caption{Order verification study of the ADI code in space and time.}
  \label{fig:OVS_ADI}
\end{figure}

For performance analysis, the cost of the ADI code is defined as the number of floating point operations (FLOPs) including additions, multiplications, and, for the baseline implementation only, divisions. Moreover, the data movement in bytes between various levels of cache was measured using the Hardware Performance Counter (HWPC) functionality in CrayPat to quantify the operational intensity.

The results of a roofline analysis are shown in Fig. \ref{fig:roofline_ADI_serial} for the baseline code. Three data series are presented, which use the same performance data but compute the operational intensity with respect to different levels of the cache heirarchy. The imlementation appears compute bound with respect to memory tranfers with RAM, but for large $N$ it becomes clearly memory bound at higher levels of the cache, albeit with still siginificant room to improve towards the memory bandwidth rooflines. Maximum performance of 0.734 FLOPs/cycle, or 18\% of peak scalar performance is observed. On the test machine, with an Intel Haswell central processing unit (CPU), the peak performance is calculated by considering the maximum throughput of 2 fused multiply add (FMA) instructions per cycle, giving 4 total FLOPs per cycle. Data for the cache bandwidths used in this (and subsequent) roofline plots was obtained from \cite{7_cpu} and \cite{manual_intel}.

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/roofline_ADI_serial.eps}
  \caption{Roofline analysis of the ADI baseline code.}
  \label{fig:roofline_ADI_serial}
\end{figure}

\mypar{RW}
One of the most important aspects of the RW implementation is the choice of pseudorandom rumber generator (PRNG) used. Knowing that parallelization would eventually be necessary, the SITMO PRNG \cite{sitmo} was initially selected. It claims to be faster than the C++ Mersenne Twister, passes all BigCrush test in the TestU01 framework, and provide easy access to many non-overlapping streams which would be useful for later parallelization. For the baseline implementation only, the code loops over all nodes and then at each node loops over all particles at that node. For each particle a random number is generated to determine if it stays or in which direction it should move.

An OVS for the baseline RW code is shown in Fig. \ref{fig:OVS_RW_1} with respect to the number of particles in the simulation, again comparing to the analytic solution. This study confirms the baseline to converge as $\sqrt{M}$ as expected from theory. As explained in \ref{subsec:RW_method}, algorithmic changes were made during later optimizations of the RW code, and so additional OVS were carried out on these later versions. Results for these verification studies are shown in Fig. \ref{fig:OVS_RW_2} and confirm that the $\sqrt{M}$ convergence is preserved.

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/OVS_RW_2.eps}
  \caption{Order verification studies of the optimized RW codes in the number of simulation particles.}
  \label{fig:OVS_RW_2}
\end{figure}

Scaling for the RW method depends on which version is being considered. The baseline implementation should scale as $O(M)$, where $M$ is the number of particles in the simulation. This is because a random number must be generated for each particle in the simulation to determine it's movement for a given timestep, independent of how many grid points there are in the simulation. The later versions that use a binomial random variable (see \ref{subsec:RW_method}) to simulate particle movement scale as $O(N^2)$ because exactly four such random variables must be evaluated for each grid point, independent of the number of particles in the simulation.

Measurements of this runtime scaling (see \ref{subsec:RW_results}, Fig. \ref{fig:runtime_RW}) confirm $O(N^2)$ for all versions of the code. This is as expected for the optimized versions, but also matches for the baseline version, since the number of particles was chosen as $M=1000N^2$, meaning the $O(M)$ complexity is equivalent to $O(N^2)$ complexity for these experiments.


% ----------------------------------------------------------------------
\section{Optimization Method}\label{sec:method}

In this section the main optimizations that were undertaken are explained. Optimizations of the ADI and RW methods are discussed in separate subsections. For each algorithm, the optimizations are broken down into three revisions: the \textit{Scalar} revision contains only optimizations that do not involve manual vecotorization or parallelization (although it is noted that the compiler was not prevented from vectorizing, so this naming scheme is not meant to imply that the machine code is not vectorized, only that it was not manually attempted at this stage); the \textit{AVX} revision then adds manual vectorization using intrinsics; and the \textit{OpenMP} revision add parallelization using OpenMP to run on a single full XC50 node of Piz Daint.

\subsection{ADI}\label{subsec:ADI_method}

\mypar{Scalar}
This revision consists mainly of 

\mypar{AVX}
In this revision we used

\mypar{OpenMP}
This revision was targeted at 


\subsection{RW}\label{subsec:RW_method}

\mypar{Scalar}
The main optimization of this version lies in no longer looping over every particle and indivdually computing a random number to determine its potential movement, but rather looping only over each node and generating four binomially distributed random numbers to determine the ensemble number of particles that move in each direction. This makes the runtime scaling independent of the number of particles in the simulation, allowing much higher accuracy simulations  without prohibitively increasing the runtime. It is noted that because the sum of the four binomially distributed numbers can be greater than the actual number of particles on the node, particularly for small $M$, a check was added to the code to handle negative nodal values during simulation, but they can still show up as negative values in the final density solution.

A further optimization involved moving the computation out of the simulation loop and into it's own function, such that the conversion was not calculated on each time step, but could be manually called after completion of the simulation to compute the final approximate solution.

\mypar{AVX}
Vectorization of the code using manual Single Instruction Multiple Data (SIMD) programming was done by leveraging Intel fused-multiply-add (FMA) and advanced vector extension (AVX) intrinsics. However, it was quickly discovered that changing the code to use AVX intrinsics had only a minimal speedup because the PRNG itself was where most of the runtime was spent, and so vectorized PRNG options were investigated. Intel's Math Kernal Library (MKL) Vector Statistics (VS) package implements a~Streaming\linebreak SIMD Extension (SSE) version of its fast mersenne twister PRNG \cite{SFMT}, and so this was integrated with the AVX intrinsics to create a fully vectorized version of the code.

Because the RW algorithm updates the four neighbouring nodes for any given one being looped over, it was found to be faster if only six nodes were looped over at a time, instead of the maximum eight which would correspond to a full AVX vector of 32-bit integers. This allowed the values being added to the neighbouring nodes to be cleverly aligned within the AVX vectors and added using basic ADD intrinsics, one on top of the other, and eliminated a~small dependency chain of loads and stores that occurred when the loop was originally unrolled by a full factor of eight.

\mypar{OpenMP}
Again because of the RW algorithm updating the nodes neighbouring the ones being looped over, the main challenge for OpenMP parallelization was preventing race conditions at the boundaries of each thread's working set. To this end, an array of {\tt omp\_mutex} objects (from the C++ class implementation provided in HPCSE I) was introduced, with each lock in the array matching to one of the working set boundaries. Working set indices for each thread were manually computed and the inner for loop was separated into three parts, one each for the first two and final two rows in each working set, which are the ones that have overlap with neighbouring threads, and one for the interior cells that do not have any possible race condition. Each thread then must acquire a lock before entering on of its boundary loops, with the lock for the first rows loop of one thread corresponding to the lock for the final rows of the previous thread to prevent race conditions.

For the PRNG, each thread has its own  PRNG stream which is {\tt threadprivate}. The streams are ensured to be non-overlapping by using the skip ahead functionality provided by the MKL VS library.


% ----------------------------------------------------------------------
\section{Optimization Results}\label{sec:results}

In this section the results of numerical benchmarks of the code at various stages are presented. Runtime and scaling experiments were performed for both algorithms, as well as performance and roofline analyses for ADI.

\mypar{Experimental setup}
All data was collected using a single XC50 node of Piz Daint, which has a 12 core Intel Xeon E5-2690 v3 running at 2.6 GHz \cite{daint}. Each physical processor core has a 64 KB L1 cache split equally into two 32 KB data and instruction caches, and a 256 KB unified L2 cache \cite{cpu_world}. The 30 MB L3 cache is shared between all cores \cite{ark_intel}, and there is 24.9 GB/s bandwidth to main memory \cite{7_cpu}. The Intel C++ Compiler v17.0.1 was used with "-O3 -std=c++11 -DNDEBUG -march=core-avx2 -fno-alias -qopenmp -mkl" flags. The Intel time stamp counter (TSC) was used for runtime measurements and CrayPat was used for cache miss measurements.

The inputs are the number of grid points in domain discretization, the timestep between iterations, and the number of timesteps computed. We used grids of various sizes, up to $N\times N=7680\times7680$. The diffusion constant $D=1$ is used for all simulations, and for RW simulations $M=1000N^2$ particles was used. For ADI, $1000$ timesteps were computed, and for RW $50$ timesteps were computed to achieve a reasonable runtime for measurements. Each simulation was repeated $10$ times, with the minimum cycle count used for the runtime and the average cache misses used for the memory transfer (the average is used only because it would have prohibitively time-consuming to measure minimum values with CrayPat).

All experiments were performed with warm cache. This is largely to simulate real world usage scenarios, where the simulation would most likely be performed immediately after initialization while the data is still potentially in cache, exactly as in these experiments. It is noted, however, that the initial cache state should have little effect on the results in either case, as the effects will be amortized over the number of iterations performed, and should become negligible for the relatively large number of iterations used.

\subsection{ADI}\label{subsec:ADI_results}
Roofline, runtime, and scaling experiments were conducted on the ADI code.

\mypar{Results}
The runtime plots of various revisions of the ADI code are shown in Fig. \ref{fig:runtime_ADI}. Each consecutive revision of the code resulted in a~speedup.

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/roofline_ADI_all.eps}
  \caption{Roofline analysis of the ADI code.}
  \label{fig:roofline_ADI_all}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/roofline_ADI_AVX.eps}
  \caption{Roofline analysis of the ADI AVX code.}
  \label{fig:roofline_ADI_AVX}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/roofline_ADI_scalar.eps}
  \caption{Roofline analysis of the ADI scalar code.}
  \label{fig:roofline_ADI_scalar}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/runtime_ADI.eps}
  \caption{Runtime analysis of the ADI code.}
  \label{fig:runtime_ADI}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/scaling_strong_ADI.eps}
  \caption{Strong Scaling analysis of the ADI OpenMP code.}
  \label{fig:scaling_strong_ADI}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/scaling_weak_ADI.eps}
  \caption{Weak Scaling analysis of the ADI OpenMP code.}
  \label{fig:scaling_weak_ADI}
\end{figure}

\subsection{RW}\label{subsec:RW_results}
Benchmarking experiments were conducted on the RW code

\mypar{Results}
Only runtime and scaling analyses of the RW code were conducted and are presented here. Performance and roofline analyses were not carried out because of the ''black box'' nature of the PRNGs in the code, which render it impossible to measure or even usefully approximate the number and type of operations which occur during the simulation.

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/runtime_RW.eps}
  \caption{Runtime analysis of the RW code.}
  \label{fig:runtime_RW}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/scaling_strong_RW.eps}
  \caption{Strong Scaling analysis of the RW OpenMP code.}
  \label{fig:scaling_strong_RW}
\end{figure}

\begin{figure}\centering
  \includegraphics[width=\linewidth]{./plots/scaling_weak_RW.eps}
  \caption{Weak Scaling analysis of the RW OpenMP code.}
  \label{fig:scaling_weak_RW}
\end{figure}


% ----------------------------------------------------------------------
\section{Conclusion}
Conclude things

% ----------------------------------------------------------------------


\begin{thebibliography}{99}

\urlstyle{same}

\bibitem{thomas}{Wikipedia. (2017, May 16). \emph{Tridiagonal matrix algorithm} [Online]. Available: \url{https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm}}

\bibitem{daint}{CSCS. (Accessed 2017, Aug). \emph{Piz Daint} [Online]. Available: \url{http://www.cscs.ch/computers/piz_daint/}}

\bibitem{7_cpu}{7-CPU. (Accessed 2017, Aug). \emph{Intel Haswell} [Online]. Available: \url{http://www.7-cpu.com/cpu/Haswell.html}}

\bibitem{manual_intel}{Intel Corporation. (2016, Jun). \emph{Intel 64 and IA-32 Architectures Optimization Reference Manual} [Online]. Available: \url{https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html}}

\bibitem{cpu_world}{CPU-World. (Accessed 2017, Aug). \emph{Intel Xeon E5-2690 v3 specifications} [Online]. Available: \url{http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%20E5-2690%20v3.html}}

\bibitem{ark_intel}{Intel Corporation. (Accessed 2017, Aug). \emph{Intel Xeon Processor E5-2690 v3} [Online]. Available: \url{http://ark.intel.com/products/81713/Intel-Xeon-Processor-E5-2690-v3-30M-Cache-2_60-GHzl}}

\bibitem{SFMT}{Intel Corporation. (Accessed 2017, Aug). \emph{SFMT19937} [Online]. Available: \url{https://software.intel.com/en-us/node/590406}}

\bibitem{transpose}{user2927848. (2016, Mar 23). \emph{m256d TRANSPOSE4 Equivalent} [Online]. Available: \url{https://stackoverflow.com/questions/36167517/m256d-transpose4-equivalent}}

\bibitem{sitmo}{T. van den Berg. (Accessed 2017, Aug). \emph{High Quality C++ Parallel Random Number Generator} [Online]. Available: \url{https://www.sitmo.com/?p=1206}}

\end{thebibliography}

\end{document}
